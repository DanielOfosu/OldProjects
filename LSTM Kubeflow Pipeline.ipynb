{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp import compiler\n",
    "from kfp import components\n",
    "from kfp.components import InputPath, InputTextFile, OutputPath, OutputTextFile\n",
    "from kfp.components import func_to_container_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(X_train_path: InputPath(str), Y_train_path: InputPath(str),\n",
    "         trained_model: OutputPath('TFModel'), epoch_amount:int = 122,batches:int = 64,num_prediction:int =3):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense\n",
    "    from tensorflow.keras.layers import LSTM\n",
    "    from tensorflow.keras.layers import Dropout\n",
    "    from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "    print('Libraries imported!')\n",
    "    \n",
    "    X_train = np.loadtxt(X_train_path)\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "    Y_train = np.loadtxt(Y_train_path)\n",
    "    \n",
    "    \n",
    "    def build_model(X_train, Y_train,epoch_amount,batches):\n",
    "        \"\"\"Builds the LSTM model\n",
    "\n",
    "        Based on the inputs given starts training the model\n",
    "        in order to be used in forecasts.\n",
    "\n",
    "        Args:\n",
    "            X_train: Training set X, which has been reshaped\n",
    "            Y_train: Training set Y, no modifications\n",
    "\n",
    "        Returns:\n",
    "            model: The final LSTM model\n",
    "            history: Model validation history\n",
    "\n",
    "        Raises:\n",
    "            Exception: Any exception in model training.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print('Training the LSTM model')\n",
    "\n",
    "            model = Sequential()\n",
    "            model.add(LSTM(100, input_shape=(X_train.shape[1],\n",
    "                                             X_train.shape[2]), recurrent_dropout=0.2))\n",
    "            model.add(Dense(60))\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(Dense(1))\n",
    "            model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "            # Verbose 0 for cleanliness\n",
    "            history = model.fit(X_train, Y_train, epochs=epoch_amount,\n",
    "                                batch_size=batches, verbose=0, shuffle=False)\n",
    "        except Exception as e:\n",
    "            print(\"Exception in model training \"+ str(e))\n",
    "        else:\n",
    "            print('LSTM Model successfully trained!')\n",
    "            model.save(trained_model)\n",
    "            \n",
    "    build_model(X_train, Y_train, epoch_amount, batches)\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(station, X_train_path: InputPath(str), model_path: InputPath('TFModel'), dataset_path: InputPath(str),\n",
    "         scaler_path: InputPath(str), df_path: InputPath(str), final_df: OutputPath(str), look_back: int, num_prediction:int =3) -> float:\n",
    "    \"\"\" \n",
    "    Function that predicts future values with given model\n",
    "    \n",
    "    Returns last actual value to help debugging.\n",
    "    \n",
    "    Saves predicted values to bigQuery table.\n",
    "    \"\"\"\n",
    "        \n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'protobuf==3.12.0'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'joblib==0.16.0'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'grpcio==1.24.3'])\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import datetime\n",
    "    from datetime import timedelta\n",
    "    from time import time\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    import time\n",
    "    from dateutil.relativedelta import relativedelta\n",
    "\n",
    "    print('Libraries imported!')\n",
    "    \n",
    "    X_train = np.loadtxt(X_train_path)\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    dataset = np.loadtxt(dataset_path)\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    df1 = pd.read_csv(df_path)\n",
    "    print(df1.tail())\n",
    "    df1['Date'] = pd.to_datetime(df1['Date'])\n",
    "    \n",
    "        \n",
    "    def forecasting(X_train,dataset,scaler,model,num_prediction):\n",
    "\n",
    "        def predict(num_prediction, model):\n",
    "                \"\"\"Creates a prediction based on the model given\n",
    "\n",
    "                Makes a forecast for the time steps given on num_prediction. \n",
    "                For example num_prediction = 3 outputs t, t+1 ,t+2 and t+3 values, \n",
    "                of which the three latter are forecasts.\n",
    "\n",
    "                Args:\n",
    "                    num_prediction: How many timesteps forward is the model predicting\n",
    "                    model: The pretrained model which is used in the prediction\n",
    "\n",
    "                Returns:\n",
    "                    prediction_list: A list of the predicted values\n",
    "\n",
    "\n",
    "                Raises:\n",
    "                    Exception: Any exception in model training.\n",
    "                \"\"\"\n",
    "                try:\n",
    "                    print('Making the predictions')\n",
    "                    prediction_list = X_train[-look_back:]\n",
    "\n",
    "                    for _ in range(num_prediction):\n",
    "                        x = prediction_list[-look_back:]\n",
    "                        x = x.reshape((1, 1, look_back))\n",
    "                        out = model.predict(x)[0][0]\n",
    "                        prediction_list = np.append(prediction_list, out)\n",
    "                    prediction_list = prediction_list[look_back-1:]\n",
    "                except Exception as e:\n",
    "                    print(\"Exception in prediction\" + str(e))\n",
    "                else:\n",
    "                    return prediction_list\n",
    "        # Reshaping the dataset in preparation to apply the forecast\n",
    "        X_train = dataset.reshape((-1))\n",
    "\n",
    "        # How many time-steps forward is being predicted.\n",
    "        forecast = predict(num_prediction, model)\n",
    "        forecast = scaler.inverse_transform([forecast])\n",
    "        print('Forecasting successful!')\n",
    "        # The first value is the last actual value.\n",
    "        # The last three values are predictions (forecasts).\n",
    "        # t, t+1 ,t+2 ,t+3\n",
    "        lastmonth = df1['Date'][len(df1) - 1].date()\n",
    "        nextmonth = lastmonth.replace(day=1) + relativedelta(months=1)\n",
    "        print(\"lastmonth\",lastmonth)\n",
    "        print(\"nextmonth:\",nextmonth)\n",
    "        final = pd.DataFrame(columns=['Date','Prediction','Model','MET_LOAD_TIME','MET_CRT_BY_PROCESS', 'Station'])\n",
    "\n",
    "        for i in range(num_prediction):\n",
    "            current = datetime.datetime.now()\n",
    "\n",
    "            final.at[i,'Date'] = nextmonth + relativedelta(months=i)\n",
    "            final.at[i,'Prediction'] = forecast[0][i+1]\n",
    "            final.at[i,'Model'] = 'LSTM'\n",
    "            final.at[i,'MET_LOAD_TIME'] = current\n",
    "            final.at[i,'MET_CRT_BY_PROCESS'] = 'LSTM Process'\n",
    "            final.at[i, 'Station']  = station\n",
    "\n",
    "        \n",
    "        final.to_csv(final_df, index=False)\n",
    "        return(forecast[0][0])\n",
    "    \n",
    "    # Function call\n",
    "    return(forecasting(X_train,dataset,scaler,model,num_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(result_path: InputPath(str)):\n",
    "    \"\"\"Writes result to BigQuery.\n",
    "    \n",
    "    Arguments: Path to result dataframe.\n",
    "    \"\"\"\n",
    "    import sys, subprocess;\n",
    "    import pandas as pd\n",
    "    import io\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas_gbq==0.13.2'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'google-auth<2.0dev,>==1.18.0'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pyarrow==0.17.1'])\n",
    "    import pandas_gbq\n",
    "    import pyarrow\n",
    "    from dateutil.relativedelta import relativedelta\n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    final = pd.read_csv(result_path)\n",
    "    print(\"Result table read, starting upload to BQ.\")\n",
    "    client = bigquery.Client()\n",
    "    table_id = 'r-instance.CL_Demand_Forecast.CL_demand_predictions'\n",
    "    # Since string columns use the \"object\" dtype, pass in a (partial) schema\n",
    "    # to ensure the correct BigQuery data type.\n",
    "    \n",
    "    job_config = bigquery.LoadJobConfig()\n",
    "    job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n",
    "    job_config.schema=[\n",
    "        bigquery.SchemaField('Date', bigquery.enums.SqlTypeNames.DATE),\n",
    "        bigquery.SchemaField('Prediction', bigquery.enums.SqlTypeNames.FLOAT),\n",
    "        bigquery.SchemaField('Model', bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField('MET_LOAD_TIME', bigquery.enums.SqlTypeNames.DATETIME),\n",
    "        bigquery.SchemaField('MET_CRT_BY_PROCESS', bigquery.enums.SqlTypeNames.STRING),\n",
    "        bigquery.SchemaField('Station', bigquery.enums.SqlTypeNames.STRING),\n",
    "    ]\n",
    "    print(str(final.to_json(orient=\"records\",lines=True)))\n",
    "    with io.StringIO(final.to_json(orient=\"records\",lines=True)) as source_file:\n",
    "        job = client.load_table_from_file(source_file, table_id, job_config=job_config)\n",
    "    \n",
    "    # Wait for the load job to complete.\n",
    "    job.result()\n",
    "    print(\"Table uploaded to bigQuery!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_data(data_path: OutputPath(str), extdata_path: OutputPath(str), datas1_path: OutputPath(str), datas2_path: OutputPath(str), datas3_path: OutputPath(str), datas4_path: OutputPath(str)):\n",
    "    \"\"\"Queries the data from BigQuery.\n",
    "\n",
    "    Loads the Neste data from BigQuery.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        df1: A pandas dataframe of the CardLock data\n",
    "\n",
    "    Raises:\n",
    "        NotFound: An error occured in loading the table from BQ.\n",
    "    \"\"\"\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas_gbq==0.13.2'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'grpcio==1.24.3'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'google-auth==1.18.0'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'google-auth<2.0dev,>==1.18.0'])\n",
    "    import pandas as pd\n",
    "    import pandas_gbq\n",
    "    from google.cloud import bigquery\n",
    "    from google.api_core.exceptions import AlreadyExists, NotFound\n",
    "    try:\n",
    "        # Loads the Neste data from BigQuery\n",
    "        # Expects that Transaction_Date is stored as datetime and Quantity as float in BigQuery.\n",
    "        print('Loading the data...')\n",
    "        client = bigquery.Client()\n",
    "        sql1 = \"\"\"\n",
    "        SELECT Transaction_Date as Date, SUM(Quantity) as Quantity \n",
    "        FROM `r-instance.CL_Demand_Forecast.CL_Demand_Forecast_Ready_SQ`\n",
    "        GROUP BY Date\n",
    "        ORDER BY DATE asc\n",
    "        \"\"\"\n",
    "\n",
    "        df1 = client.query(sql1).to_dataframe()\n",
    "        df1 = df1.rename(columns={\"Quantity\": \"Amount\"})\n",
    "        \n",
    "        # Loads the Historical West Coast Demand data from BigQuery\n",
    "        sql2 = \"\"\"\n",
    "        SELECT * FROM `r-instance.CL_Demand_Forecast.Historical_DieselSales_WestCoast` \n",
    "        ORDER BY DATE asc\n",
    "        \"\"\"\n",
    "        df2 = client.query(sql2).to_dataframe()\n",
    "        \n",
    "\n",
    "        sql_station_1 = \"\"\"\n",
    "        SELECT Transaction_Date as Date, SUM(Quantity) as Amount \n",
    "        FROM `r-instance.CL_Demand_Forecast.CL_Demand_Forecast_Ready_SQ`\n",
    "        WHERE Site_Address LIKE '2709%'\n",
    "        GROUP BY Date\n",
    "        ORDER BY DATE asc\n",
    "        \"\"\"\n",
    "        sql_station_2 = \"\"\"\n",
    "        SELECT Transaction_Date as Date, SUM(Quantity) as Amount\n",
    "        FROM `r-instance.CL_Demand_Forecast.CL_Demand_Forecast_Ready_SQ`\n",
    "        WHERE Site_Address LIKE '1790%'\n",
    "        GROUP BY Date\n",
    "        ORDER BY DATE asc\n",
    "        \"\"\"\n",
    "        sql_station_3 = \"\"\"\n",
    "        SELECT Transaction_Date as Date, SUM(Quantity) as Amount \n",
    "        FROM `r-instance.CL_Demand_Forecast.CL_Demand_Forecast_Ready_SQ`\n",
    "        WHERE Site_Address LIKE '5675%'\n",
    "        GROUP BY Date\n",
    "        ORDER BY DATE asc\n",
    "        \"\"\"\n",
    "        sql_station_4 = \"\"\"\n",
    "        SELECT Transaction_Date as Date, SUM(Quantity) as Amount \n",
    "        FROM `r-instance.CL_Demand_Forecast.CL_Demand_Forecast_Ready_SQ`\n",
    "        WHERE Site_Address LIKE '816%'\n",
    "        GROUP BY Date\n",
    "        ORDER BY DATE asc\n",
    "        \"\"\"\n",
    "        client = bigquery.Client()\n",
    "        df_station1 = client.query(sql_station_1).to_dataframe()\n",
    "        df_station2 = client.query(sql_station_2).to_dataframe()\n",
    "        df_station3 = client.query(sql_station_3).to_dataframe()\n",
    "        df_station4 = client.query(sql_station_4).to_dataframe()\n",
    "        print('Data loaded')\n",
    "\n",
    "    except NotFound:\n",
    "        print(\"An error occured in loading the tables from BigQuery: TABLE NOT FOUND\")\n",
    "    else:\n",
    "        #return df1, df2\n",
    "        df1.to_csv(data_path, index=False)\n",
    "        df2.to_csv(extdata_path, index=False)\n",
    "        df_station1.to_csv(datas1_path, index=False)\n",
    "        df_station2.to_csv(datas2_path, index=False)\n",
    "        df_station3.to_csv(datas3_path, index=False)\n",
    "        df_station4.to_csv(datas4_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " def process_data(station, look_back, data_path: InputPath(str), datas1_path: InputPath(str), datas2_path: InputPath(str), datas3_path: InputPath(str), datas4_path: InputPath(str), edata_path: InputPath(str), xtrain_path: OutputPath(str), ytrain_path: OutputPath(str),\n",
    "                  dset_path: OutputPath(str), scaler_path: OutputPath(str), df_path: OutputPath(str)):\n",
    "    \"\"\"Preprocess the data to be ready for applying the LSTM model\n",
    "\n",
    "    Args:\n",
    "        df1: A pandas dataframe of the Cardlock company data\n",
    "        df2: A pandas dataframe of the historical US data\n",
    "        look_back: How many months of data the model needs for the predictions\n",
    "\n",
    "    Returns:\n",
    "        X_train: Training set X, which has been reshaped\n",
    "        Y_train: Training set Y, no modifications\n",
    "        dataset: The full dataset\n",
    "        scaler: The scaler used, which can be used later on to invert the scaling\n",
    "\n",
    "    Raises:\n",
    "        IndexError: Sequence subscript is out of range.\n",
    "    \"\"\"\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'joblib==0.16.0'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'grpcio==1.24.3'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pyarrow==0.17.1'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas_gbq==0.13.2'])\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    #import datetime\n",
    "    import joblib\n",
    "    #from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    import time\n",
    "    import pyarrow\n",
    "    from google.cloud import bigquery\n",
    "    from google.api_core.exceptions import AlreadyExists, NotFound\n",
    "    \n",
    "\n",
    "    def check_values(df):  \n",
    "        if any(df['Amount'] < 0):\n",
    "            print('WARNING: There are negative values in the data!')\n",
    "\n",
    "        if any(df['Amount'] > 1000000):   #values over 1 million\n",
    "            print('WARNING: The data contains values over a million.')\n",
    "        else:\n",
    "            print('No outlier values found!')\n",
    "    \n",
    "    try:\n",
    "        print('Preprocessing the data for LSTM modeling')\n",
    "        df1 = pd.read_csv(data_path)\n",
    "        df2 = pd.read_csv(edata_path)\n",
    "        df_station1 = pd.read_csv(datas1_path)\n",
    "        df_station2 = pd.read_csv(datas2_path)\n",
    "        df_station3 = pd.read_csv(datas3_path)\n",
    "        df_station4 = pd.read_csv(datas4_path)\n",
    "        \n",
    "        stations = [\"2709 Teagarden Street San Leandro CA 94577\",\"1790 S. 10th Street San Jose CA 95112\",\n",
    "                   \"5675 7th Street Keyes CA 95328\",\"816 S. Frontage Road Ripon CA 95366\"]\n",
    "        \n",
    "        if(station==\"2709 Teagarden Street San Leandro CA 94577\"):\n",
    "            df1 = df_station1\n",
    "        if(station==\"1790 S. 10th Street San Jose CA 95112\"):\n",
    "            df1 = df_station2\n",
    "        if(station==\"5675 7th Street Keyes CA 95328\"):\n",
    "            df1 = df_station3\n",
    "        if(station==\"816 S. Frontage Road Ripon CA 95366\"):\n",
    "            df1 = df_station4\n",
    "        \n",
    "        print(\"Dataframes ready.\")\n",
    "        \n",
    "        # The next part is for checking if the last month has full data or not.\n",
    "\n",
    "\n",
    "        # Sets the latest month as being full\n",
    "        latest_month_full = True\n",
    "\n",
    "        # If the latest month is not full, set latest_month_full to False.\n",
    "        def check_full_month(df):\n",
    "            import datetime\n",
    "            import calendar\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            df = df.set_index('Date')\n",
    "            df = df.asfreq(df.index.freq)\n",
    "            \n",
    "            df['day'] = df.index.day\n",
    "            last_date = df['day'].iloc[-1]\n",
    "            \n",
    "            # Marks the month as being full if the date is 29 30 or 31\n",
    "            # TODO: Check the last date properly\n",
    "            if last_date < 28:\n",
    "                return False\n",
    "     \n",
    "        latest_month_full_station1 = check_full_month(df_station1)\n",
    "        latest_month_full_station2 = check_full_month(df_station2)\n",
    "        latest_month_full_station3 = check_full_month(df_station3)\n",
    "        latest_month_full_station4 = check_full_month(df_station4)\n",
    "   \n",
    "        check_values(df1)\n",
    "\n",
    "        def create_dataset(dataset, look_back):\n",
    "            X, Y = [], []\n",
    "            for i in range(len(dataset)-int(look_back)-1):\n",
    "                a = dataset[i:(i+int(look_back)), 0]\n",
    "                X.append(a)\n",
    "                Y.append(dataset[i + int(look_back), 0])\n",
    "            return np.array(X), np.array(Y)\n",
    "        \n",
    "        if station in stations:\n",
    "            latest_month_full = check_full_month(df1)\n",
    "        else:\n",
    "            # Drops the last row if last month is not full of data\n",
    "            if latest_month_full_station1 == False or latest_month_full_station2 == False or latest_month_full_station3 == False or latest_month_full_station4 == False:\n",
    "                latest_month_full = False\n",
    "\n",
    "        # Modifies the Neste data to Monthly data through resampling\n",
    "        df1['Date'] = pd.to_datetime(df1['Date'])\n",
    "        df1 = df1.set_index('Date')\n",
    "        df1 = df1.asfreq(df1.index.freq)\n",
    "        df1 = df1.resample('M').sum()\n",
    "        \n",
    "\n",
    "        if latest_month_full == False:\n",
    "            df1 = df1[:-1]\n",
    "        \n",
    "        # Dropping unneeded dates\n",
    "        df2 = df2.set_index('Date')\n",
    "        df2 = df2[-168:-27]\n",
    "        df2 = df2.sort_index()\n",
    "\n",
    "        # TODO: This row drops the last 5 rows due to missing data. Remove in final version\n",
    "        #df1 = df1[:-5]\n",
    "        # Dropping the first 11 rows in order to get a more accurate representation of the real situation\n",
    "        df1 = df1[11:]\n",
    "        print(df1.tail(5))\n",
    "        # Placeholders for debugging\n",
    "        # df2.to_csv('LSTM_debug/df2.csv', index = True, header=True)\n",
    "        # df1.to_csv('LSTM_debug/df1.csv', index = True, header=True)\n",
    "        \n",
    "        #testchanges\n",
    "        dfmodified = df1.reset_index()\n",
    "        dfmodified.to_csv(df_path, index=False)\n",
    "        # Scaling both of the datasets with the MinMaxScaler\n",
    "        historical_data = df2.values  # numpy.ndarray\n",
    "        historical_data = historical_data.astype('float32')\n",
    "        historical_data = np.reshape(historical_data, (-1, 1))\n",
    "        scaler2 = MinMaxScaler(feature_range=(0, 1))\n",
    "        historical_data = scaler2.fit_transform(historical_data)\n",
    "\n",
    "        neste_data = df1.values  # numpy.ndarray\n",
    "        neste_data = neste_data.astype('float32')\n",
    "        neste_data = np.reshape(neste_data, (-1, 1))\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        neste_data = scaler.fit_transform(neste_data)\n",
    "\n",
    "        # Combining the scaled datasets\n",
    "        dataset = np.concatenate([historical_data, neste_data])\n",
    "        # pd.DataFrame(dataset).to_csv(\"test.csv\")\n",
    "\n",
    "        X_train, Y_train = create_dataset(dataset, look_back)\n",
    "        # Reshaping input to be [samples, time steps, features]\n",
    "        #X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "        \n",
    "\n",
    "    except IndexError:\n",
    "        print(\"Out of index: look back time is too long\")\n",
    "    else:\n",
    "        print('Data successfully preprocessed')\n",
    "        np.savetxt(xtrain_path, X_train)\n",
    "        np.savetxt(ytrain_path, Y_train)\n",
    "        np.savetxt(dset_path, dataset)\n",
    "        joblib.dump(scaler, scaler_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dataset_path: InputPath(str), scaler_path: InputPath(str), look_back: int, epoch_amount:int = 122,batches:int = 64) -> float:\n",
    "    \"\"\"Function to evaluate model performance with past data.\n",
    "    \n",
    "    Conduct a train-test split and evaluate performance with test data.\n",
    "    \n",
    "    Args: Some data\n",
    "    \n",
    "    Returns: MAE metric for the test period.\n",
    "    \"\"\"\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'joblib==0.16.0'])\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense\n",
    "    from tensorflow.keras.layers import LSTM\n",
    "    from tensorflow.keras.layers import Dropout\n",
    "    from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    \n",
    "    print(\"Libraries imported!\")\n",
    "    \n",
    "    scaler = joblib.load(scaler_path)\n",
    "    dataset = np.loadtxt(dataset_path)\n",
    "    print(dataset.shape)\n",
    "    print(len(dataset))\n",
    "    train_size = int(len(dataset) * 0.80)\n",
    "    test_size = len(dataset) - train_size\n",
    "    train, test = dataset[0:train_size], dataset[train_size:len(dataset)]\n",
    "    def create_dataset(dataset, look_back=18):\n",
    "        X, Y = [], []\n",
    "        for i in range(len(dataset)-look_back-1):\n",
    "            a = dataset[i:(i+look_back)]\n",
    "            X.append(a)\n",
    "            Y.append(dataset[i + look_back])\n",
    "        return np.array(X), np.array(Y)\n",
    "    print(\"Creating dataset!\")\n",
    "    #look_back = 5\n",
    "    X_train, Y_train = create_dataset(train, look_back)\n",
    "    X_test, Y_test = create_dataset(test, look_back)\n",
    "\n",
    "    # reshape input to be [samples, time steps, features]\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "    print(\"Reshaping train.\")\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "    print(\"Reshaping test.\")\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(X_train.shape[1],\n",
    "                                     X_train.shape[2]), recurrent_dropout=0.2))\n",
    "    model.add(Dense(60))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    # Verbose 0 for cleanliness\n",
    "    history = model.fit(X_train, Y_train, epochs=epoch_amount,\n",
    "                        batch_size=batches, verbose=0, shuffle=False)\n",
    "    print(\"Model fitted.\")\n",
    "\n",
    "    test_predict = model.predict(X_test)\n",
    "    test_predict = scaler.inverse_transform(test_predict)\n",
    "    Y_test = scaler.inverse_transform([Y_test])\n",
    "    print('Test Mean Absolute Error:', mean_absolute_error(Y_test[0], test_predict[:,0]))\n",
    "    print('Test Root Mean Squared Error:',np.sqrt(mean_squared_error(Y_test[0], test_predict[:,0])))\n",
    "    return(mean_absolute_error(Y_test[0], test_predict[:,0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the functions to pipeline operations.\n",
    "query_op = components.func_to_container_op(\n",
    "    query_data,\n",
    "    base_image='jupyter/tensorflow-notebook:54462805efcb'\n",
    ")\n",
    "\n",
    "process_data_op = components.func_to_container_op(\n",
    "    process_data,\n",
    "    base_image='jupyter/tensorflow-notebook:54462805efcb'\n",
    ")\n",
    "\n",
    "build_model_op = components.func_to_container_op(\n",
    "    build_model,\n",
    "    base_image='jupyter/tensorflow-notebook:54462805efcb'\n",
    ")\n",
    "predict_op = components.func_to_container_op(\n",
    "    forecast,\n",
    "    base_image='jupyter/tensorflow-notebook:54462805efcb'\n",
    ")\n",
    "write_op = components.func_to_container_op(\n",
    "    write_data,\n",
    "    base_image='jupyter/tensorflow-notebook:54462805efcb'\n",
    ")\n",
    "\n",
    "evaluate_op = components.func_to_container_op(\n",
    "    evaluate_model,\n",
    "    base_image='jupyter/tensorflow-notebook:54462805efcb'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline.\n",
    "@dsl.pipeline(\n",
    "   name='LSTM pipeline',\n",
    "   description='A pipeline that creates a forward forecasting prediction with LSTM modelling.'\n",
    ")\n",
    "def lstm_pipeline(\n",
    "    epoch_amount: int =122,\n",
    "    batches: int =64,\n",
    "    num_prediction: int =3,\n",
    "    look_back: int =18,\n",
    "    station: str=\"all\"\n",
    "):\n",
    "    data = query_op()\n",
    "    processed = process_data_op(station, look_back, data.outputs['data'], data.outputs['datas1'], data.outputs['datas2'], data.outputs['datas3'], data.outputs['datas4'], data.outputs['extdata'])\n",
    "    mae = evaluate_op(processed.outputs['dset'], processed.outputs['scaler'], look_back, epoch_amount, batches)\n",
    "    model = build_model_op(processed.outputs['xtrain'], processed.outputs['ytrain'], \n",
    "                           look_back,epoch_amount,batches)\n",
    "    \n",
    "    \n",
    "    preds = predict_op(station, processed.outputs['xtrain'], model.outputs['trained_model'], processed.outputs['dset'],\n",
    "                           processed.outputs['scaler'], processed.outputs['df'], look_back=look_back, num_prediction=num_prediction)\n",
    "    data.execution_options.caching_strategy.max_cache_staleness = \"P1D\"\n",
    "    processed.execution_options.caching_strategy.max_cache_staleness = \"P1D\"\n",
    "    mae.execution_options.caching_strategy.max_cache_staleness = \"P1D\"\n",
    "    model.execution_options.caching_strategy.max_cache_staleness = \"P1D\"\n",
    "    preds.execution_options.caching_strategy.max_cache_staleness = \"P1D\"\n",
    "    write_op(preds.outputs['final_df'])\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ApiException",
     "evalue": "(504)\nReason: Gateway Timeout\nHTTP response headers: HTTPHeaderDict({'Content-Length': '1455', 'Content-Type': 'text/html; charset=utf-8', 'Date': 'Fri, 17 Jul 2020 05:33:18 GMT', 'X-Content-Type-Options': 'nosniff', 'X-Frame-Options': 'SAMEORIGIN', 'X-Xss-Protection': '0', 'Set-Cookie': 'S=cloud_datalab_tunnel=EsKyEQVqYFMCXgNLBiE-ZCAwQu51I9OyiFwvKVX0ouI; Path=/; Max-Age=3600'})\nHTTP response body: \n<!DOCTYPE html>\n<html lang=en>\n  <meta charset=utf-8>\n  <meta name=viewport content=\"initial-scale=1, minimum-scale=1, width=device-width\">\n  <title>Error 504 (Gateway Timeout)!!1</title>\n  <style>\n    *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/logos/errorpage/error_logo-150x54.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/logos/errorpage/error_logo-150x54-2x.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/logos/errorpage/error_logo-150x54-2x.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/logos/errorpage/error_logo-150x54-2x.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}\n  </style>\n  <a href=//www.google.com/><span id=logo aria-label=Google></span></a>\n  <p><b>504.</b> <ins>That’s an error.</ins>\n  <p>  <ins>That’s all we know.</ins>\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mApiException\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f7b72a1d5003>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkfpclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LSTM model experiment'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkfpclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LSTM run test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lstm_pipeline.tar.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/kfp/_client.py\u001b[0m in \u001b[0;36mcreate_experiment\u001b[0;34m(self, name, description, namespace)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         resource_references=resource_references)\n\u001b[0;32m--> 279\u001b[0;31m       \u001b[0mexperiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experiment_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/kfp_server_api/api/experiment_service_api.py\u001b[0m in \u001b[0;36mcreate_experiment\u001b[0;34m(self, body, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_experiment_with_http_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: E501\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_experiment_with_http_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: E501\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/kfp_server_api/api/experiment_service_api.py\u001b[0m in \u001b[0;36mcreate_experiment_with_http_info\u001b[0;34m(self, body, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0m_preload_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_preload_content'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0m_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_request_timeout'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             collection_formats=collection_formats)\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdelete_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: E501\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/kfp_server_api/api_client.py\u001b[0m in \u001b[0;36mcall_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    328\u001b[0m                                    \u001b[0mresponse_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                                    \u001b[0m_return_http_data_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollection_formats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m                                    _preload_content, _request_timeout)\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             thread = self.pool.apply_async(self.__call_api, (resource_path,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/kfp_server_api/api_client.py\u001b[0m in \u001b[0;36m__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mpost_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpost_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0m_preload_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_preload_content\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             _request_timeout=_request_timeout)\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/kfp_server_api/api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    371\u001b[0m                                          \u001b[0m_preload_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_preload_content\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                                          \u001b[0m_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_request_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                                          body=body)\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"PUT\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             return self.rest_client.PUT(url,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/kfp_server_api/rest.py\u001b[0m in \u001b[0;36mPOST\u001b[0;34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    273\u001b[0m                             \u001b[0m_preload_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_preload_content\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                             \u001b[0m_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_request_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m                             body=body)\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     def PUT(self, url, headers=None, query_params=None, post_params=None,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/kfp_server_api/rest.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m299\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mApiException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_resp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mApiException\u001b[0m: (504)\nReason: Gateway Timeout\nHTTP response headers: HTTPHeaderDict({'Content-Length': '1455', 'Content-Type': 'text/html; charset=utf-8', 'Date': 'Fri, 17 Jul 2020 05:33:18 GMT', 'X-Content-Type-Options': 'nosniff', 'X-Frame-Options': 'SAMEORIGIN', 'X-Xss-Protection': '0', 'Set-Cookie': 'S=cloud_datalab_tunnel=EsKyEQVqYFMCXgNLBiE-ZCAwQu51I9OyiFwvKVX0ouI; Path=/; Max-Age=3600'})\nHTTP response body: \n<!DOCTYPE html>\n<html lang=en>\n  <meta charset=utf-8>\n  <meta name=viewport content=\"initial-scale=1, minimum-scale=1, width=device-width\">\n  <title>Error 504 (Gateway Timeout)!!1</title>\n  <style>\n    *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/logos/errorpage/error_logo-150x54.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/logos/errorpage/error_logo-150x54-2x.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/logos/errorpage/error_logo-150x54-2x.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/logos/errorpage/error_logo-150x54-2x.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}\n  </style>\n  <a href=//www.google.com/><span id=logo aria-label=Google></span></a>\n  <p><b>504.</b> <ins>That’s an error.</ins>\n  <p>  <ins>That’s all we know.</ins>\n\n"
     ]
    }
   ],
   "source": [
    "# Compile the pipeline\n",
    "compiler.Compiler().compile(lstm_pipeline, 'lstm_pipeline.tar.gz')\n",
    "\n",
    "# Pipeline Argument Values\n",
    "arguments = {'epoch_amount': '122', 'batches': '64', 'num_prediction': '3', 'look_back': '18', 'station': '816 S. Frontage Road Ripon CA 95366'}\n",
    "\n",
    "kfpclient = kfp.Client(host='4a412867b9423117-dot-europe-west1.pipelines.googleusercontent.com')\n",
    "\n",
    "\n",
    "exp = kfpclient.create_experiment(name='LSTM model experiment')\n",
    "run = kfpclient.run_pipeline(exp.id, 'LSTM run test', 'lstm_pipeline.tar.gz', params=arguments)\n",
    "\n",
    "\n",
    "\n",
    "# The generated link below leads to the pipeline run information page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "r-cpu.3-6.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.3-6:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
